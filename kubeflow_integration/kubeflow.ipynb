{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Installing Required Software\n",
    "\n",
    "We need to make the following software available on your computer:\n",
    "* gcloud: access to Google’s cloud services\n",
    "* kubectl: controlling a Kubernetes cluster\n",
    "* kustomize: a helper tool that makes it easier to modify kubernetes jobs\n",
    "* kfctl: controlling Kubeflow specifically\n",
    "\n",
    "This tutorial is largely based on [Google's end-to-end Kubeflow tutorial](https://www.kubeflow.org/docs/gke/gcp-e2e/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gcloud, Kubectl and kusomize\n",
    "Unfortunately you cannot install gcloud through Jupyter.  If you are on a Mac you will have to run the following from the command shell to instal gcloud:\n",
    "```\n",
    "curl https://sdk.cloud.google.com | bash  # install gcloud\n",
    "exec -l $SHELL  # restart the shell\n",
    "gcloud init\n",
    "```\n",
    "If you are not on a Mac check out instructions for other systems [here](https://cloud.google.com/sdk/docs/downloads-interactive).\n",
    "\n",
    "Once you have  gcloud installed you can install kubectl and kustomize like this (again, on a Mac):\n",
    "```\n",
    "gcloud components install kubectl\n",
    "brew install kustomize\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kfctl\n",
    "We don’t “install” kfctl exactly - we just download the executable and put it in a place we can reference.  Download the appropriate version from the [Kubeflow releases page](https://github.com/kubeflow/kubeflow/releases/) into the directory of your choice, unzip it, and then put it on your path.  Example commands (in this case just putting it in the working directory) are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://github.com/kubeflow/kubeflow/releases/download/v0.5.1/kfctl_v0.5.1_darwin.tar.gz\n",
    "!tar -xvf kfctl_v0.5.1_darwin.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Creating the Project\n",
    "Google Cloud divides things into \"projects\" that can have multiple resources assocaited with them.  If you already have a project you can just use it.  In this section we will create a new project and configure gcloud to point to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use gcloud to tell Google who you are and associate all of this with your google email address.  This command will open up a browser window where you can log in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then go to the [Google Cloud console page](https://console.cloud.google.com), create a project, and get its ID.  Also **make sure billing is enabled for it**, and choose a geographical region and zone for the project.  Set those decisions as python variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='kubeflow-245520'\n",
    "REGION='us-west2'\n",
    "ZONE='us-west2-c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then configure gcloud to point to that project and zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/zone].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT\n",
    "!gcloud config set compute/zone $ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Create GKE Cluster\n",
    "Now we will create a Google Kubernetes Engine (GKE) cluster, under the umbrella of the current project, that has kubeflow running on it.\n",
    "\n",
    "Choose a name for the Kubernetes cluster to use, a name for the GCloud deployment, and login credentials for the web UI of the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFAPP='kfapp2'\n",
    "KUBEFLOW_USERNAME='fcady'\n",
    "KUBEFLOW_PASSWORD='mypass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use kfctl to create the cluster (and a directory of the same name on your local box) and set up its configurations (this will fail if you don’t have billing enabled).  This process will take a while (maybe 20 minutes?) as the resources for the cluster get provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KUBEFLOW_USERNAME=fcady\n",
      "env: KUBEFLOW_PASSWORD=mypass\n",
      "/bin/sh: ./kfctl: No such file or directory\n",
      "[Errno 2] No such file or directory: 'kfapp2'\n",
      "/Users/fieldcady/Desktop/model-deployment/kubeflow_integration\n",
      "/bin/sh: ../kfctl: No such file or directory\n",
      "/bin/sh: ../kfctl: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%env KUBEFLOW_USERNAME=$KUBEFLOW_USERNAME\n",
    "%env KUBEFLOW_PASSWORD=$KUBEFLOW_PASSWORD\n",
    "!./kfctl init $KFAPP --platform gcp --project $PROJECT --use_basic_auth -V\n",
    "%cd $KFAPP\n",
    "!../kfctl generate all -V --zone $ZONE\n",
    "!../kfctl apply all -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cluster starts you can see it at ```https://${KFAPP}.endpoints.${PROJECT}.cloud.goog/```\n",
    "\n",
    "Next connect to Kubeflow cluster+deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for kfapp2.\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!gcloud container clusters get-credentials $KFAPP --zone $ZONE --project $PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Make Storage Bucket\n",
    "Now we will set up a cloud storage location where our trained model will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=PROJECT + '-' + KFAPP + '-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west2-c gs://kubeflow-245520-kfapp2-bucket\n",
      "Creating gs://kubeflow-245520-kfapp2-bucket/...\n"
     ]
    }
   ],
   "source": [
    "!echo $ZONE gs://$BUCKET_NAME\n",
    "!gsutil mb -c regional -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the bucket listed at ```https://console.cloud.google.com/storage/browser?project=<PROJECT>```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Get code, build training Docker image, and Push it to the Registry\n",
    "The next step for getting Kubeflow running is to get the Docker training image built and stored in GCE where Kubeflow can find it.\n",
    "\n",
    "This is where you will start changing things for your own projects, but for this walk-through we will use the MNIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/kubeflow-245520/kfapp2-train:1565117819\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "VERSION_TAG=str(round(time.time()))\n",
    "TRAIN_IMG_PATH=f\"gcr.io/{PROJECT}/{KFAPP}-train:{VERSION_TAG}\"\n",
    "WORKING_DIR=\"/Users/fieldcady/Desktop/examples/mnist\"\n",
    "print(TRAIN_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/kubeflow/examples.git\n",
    "!docker build -f examples/mnist/Dockerfile.model -t $TRAIN_IMG_PATH examples/mnist\n",
    "!gcloud auth configure-docker --quiet\n",
    "!docker push $TRAIN_IMG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should then be able to see the image in the Image Registry in gcloud at\n",
    "\n",
    "```https://console.cloud.google.com/gcr/images/<PROJECT>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Create the training job and run it on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we cd into the directory with the configuration files for training and use kustomize to make some edits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd examples/mnist/training/GCS\n",
    "!kustomize edit add configmap mnist-map-training --from-literal=secretName=user-gcp-sa\n",
    "!kustomize edit add configmap mnist-map-training --from-literal=secretMountPath=/var/secrets\n",
    "!kustomize edit add configmap mnist-map-training --from-literal=GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/user-gcp-sa.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we **should** be one command away from launching the training job on our Kubeflow cluster.\n",
    "\n",
    "Alas, Kubeflow is an early-stage open-source project, and that means some rough edges.  In this case there are bugs that improperly handle the example configuration files, and we will need to edit them ourselves.  The two files that need editing are kustomization.yaml in the current directory (\"main file\") and kustomization.yaml in the base directory (\"base file\").  The changes that need to be made are:\n",
    "* In the main file the file Chief_patch.yaml gets added as a path.  But it needs a namespace associated with it that matches the one in the base file.  Do that by adding “namespace: kubeflow” to the properties underneath it.\n",
    "* The main and base files both have a \"vars:\" section that defines some properties.  Move all of the properties in the base file to the main file, deleting that section in the base file.\n",
    "\n",
    "For simplicity, the next cell will write versions of these files with those changes already made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git checkout ../base/kustomization.yaml\n",
    "!git checkout kustomization.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file = \"\"\"apiVersion: kustomize.config.k8s.io/v1beta1\n",
    "kind: Kustomization\n",
    "\n",
    "resources:\n",
    "- Chief.yaml\n",
    "\n",
    "namespace: kubeflow\n",
    "\n",
    "generatorOptions:\n",
    "  disableNameSuffixHash: true\n",
    "\n",
    "configurations:\n",
    "- params.yaml\n",
    "\"\"\"\n",
    "\n",
    "main_file = f\"\"\"apiVersion: kustomize.config.k8s.io/v1beta1\n",
    "kind: Kustomization\n",
    "\n",
    "\n",
    "configurations:\n",
    "- params.yaml\n",
    "\n",
    "# TBD (jinchihe) Need move the image to base file once.\n",
    "# the issue addressed: kubernetes-sigs/kustomize/issues/1040\n",
    "# TBD (jinchihe) Need to update the image once\n",
    "# the issue addressed: kubeflow/testing/issues/373\n",
    "images:\n",
    "- name: training-image\n",
    "  newName: {TRAIN_IMG_PATH}\n",
    "\n",
    "vars:\n",
    "- fieldref:\n",
    "    fieldPath: data.name\n",
    "  name: trainingName\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.modelDir\n",
    "  name: modelDir\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.exportDir\n",
    "  name: exportDir\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.trainSteps\n",
    "  name: trainSteps\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.batchSize\n",
    "  name: batchSize\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.learningRate\n",
    "  name: learningRate\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.GOOGLE_APPLICATION_CREDENTIALS\n",
    "  name: GOOGLE_APPLICATION_CREDENTIALS\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.secretName\n",
    "  name: secretName\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.secretMountPath\n",
    "  name: secretMountPath\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "\n",
    "patchesJson6902:\n",
    "- path: Chief_patch.yaml\n",
    "  target:\n",
    "    group: kubeflow.org\n",
    "    kind: TFJob\n",
    "    name: $(trainingName)\n",
    "    namespace: kubeflow\n",
    "    version: v1beta2\n",
    "resources:\n",
    "- ../base\n",
    "configMapGenerator:\n",
    "- literals:\n",
    "  - name=mnist-train-dist4\n",
    "  - trainSteps=20000\n",
    "  - batchSize=1000\n",
    "  - learningRate=0.01\n",
    "  - secretName=user-gcp-sa\n",
    "  - secretMountPath=/var/secrets\n",
    "  - GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/user-gcp-sa.json\n",
    "  - modelDir=gs://{BUCKET_NAME}/\n",
    "  - exportDir=gs://{BUCKET_NAME}/export\n",
    "  name: mnist-map-training\n",
    "\"\"\"\n",
    "\n",
    "_=open('../base/kustomization.yaml', 'w').write(base_file)\n",
    "_=open('kustomization.yaml', 'w').write(main_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the name=mnist-train-dist line in the YAML file.  You will have to choose a new name every\n",
    "time you re-kick-off the training job - otherwise the workflow will not get created.  Now we can kick off the job with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap \"mnist-map-training-cd2h45k8dm\" created\n",
      "tfjob.kubeflow.org \"mnist-train-dist4\" created\n"
     ]
    }
   ],
   "source": [
    "!kustomize build . | kubectl apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the workflow on the [Google Worklow Page](https://console.cloud.google.com/kubernetes/workload), where you should see a workflow like \"mnist-train-dist-chief-0\".  **Wait for it to finish.**  After it finishes running then you can copy the exported model file (which is a SavedModel in Tensorflow) to your local machine and zip it up for transfer to Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Copy trained model to local space\n",
    "!gsutil cp -r gs://$BUCKET_NAME/export .\n",
    "# Make model/ directory, deleting if it already exists\n",
    "!rm -rf model*\n",
    "!mkdir model\n",
    "# Compress the trained model into a ZIP file\n",
    "# NOTE: this expects there to be only one model in yoru export/ folder\n",
    "!cp -r export/$(ls export)/* model/\n",
    "!zip model.zip -r model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: Loading Model to Algorithmia and Creating an Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Put zipped model into Algorithmia\n",
    "!algo rm .my/kubeflow_example force=true\n",
    "!algo mkdir .my/kubeflow_example\n",
    "!algo cp model.zip data://.my/kubeflow_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then go to [Algorithmia](http://www.algorithmia.com), create a new algorithm (**making sure to give it internet access** for this tutorial), and set its name as a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMIA_USERNAME='fcady'\n",
    "ALGORITHM_NAME=\"foo\"\n",
    "\n",
    "FILE_TO_WRITE=ALGORITHM_NAME+'/src/'+ALGORITHM_NAME+'.py'\n",
    "FILE_TO_COMMIT='src/'+ALGORITHM_NAME+'.py'\n",
    "REPO_TO_CLONE=ALGORITHMIA_USERNAME+'/'+ALGORITHM_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use git to checkout this algorithm, write some example code that uses out model, and push it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning https://git.algorithmia.com/git/fcady/foo.git\n",
      "Cloning into 'foo'...\n",
      "remote: Counting objects: 255, done\u001b[K\n",
      "remote: Finding sources: 100% (255/255)\u001b[K\u001b[K\n",
      "remote: Getting sizes: 100% (55/55)\u001b[K\u001b[K\n",
      "remote: Total 255 (delta 138), reused 255 (delta 138)\u001b[K   \n",
      "Receiving objects: 100% (255/255), 11.58 MiB | 9.22 MiB/s, done.\n",
      "Resolving deltas: 100% (138/138), done.\n",
      "/Users/fieldcady/Desktop/kube/foo\n",
      "Already up to date.\n",
      "/Users/fieldcady/Desktop/kube\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!rm -rf $ALGORITHM_NAME  # Delete previously downloaded clone, if it exists\n",
    "!algo clone $REPO_TO_CLONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting foo/src/foo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $FILE_TO_WRITE\n",
    "import Algorithmia\n",
    "import tensorflow as tf\n",
    "import requests, zipfile\n",
    "from os import mkdir, listdir\n",
    "\n",
    "IMAGE_FNAME  = '/tmp/foo.png'\n",
    "\n",
    "client = Algorithmia.client()\n",
    "\n",
    "def extract_model():\n",
    "    filename = \"data://.my/kubeflow_example/model.zip\"\n",
    "    input_zip = client.file(filename).getFile().name\n",
    "    mkdir(\"/tmp/unzipped_files\")\n",
    "    zipped_file = zipfile.ZipFile(input_zip)\n",
    "    return zipped_file.extractall(\"/tmp/unzipped_files\")\n",
    "\n",
    "def download_image(url):\n",
    "    with open(IMAGE_FNAME, 'wb') as f:\n",
    "        f.write(requests.get(url).content)\n",
    "\n",
    "def create_session(path_to_graph = \"/tmp/unzipped_files/model\"):\n",
    "    session = tf.Session()\n",
    "    tf.saved_model.loader.load(session, ['serve'], path_to_graph)\n",
    "    y = session.graph.get_tensor_by_name('Softmax:0')\n",
    "    x = session.graph.get_tensor_by_name('Placeholder:0')\n",
    "    return (y, x, session)\n",
    "\n",
    "extract_model()\n",
    "Y, X, SESSION = create_session()\n",
    "\n",
    "def classify_image():\n",
    "    img = tf.keras.preprocessing.image.load_img(IMAGE_FNAME).resize((28,28))\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    xx = x.mean(axis=2).reshape((1,28,28)) / 255\n",
    "    predict_values = tf.argmax(Y, 1)\n",
    "    ret = predict_values.eval(session=SESSION,feed_dict={X: xx})\n",
    "    return int(ret)\n",
    "\n",
    "def apply(input):\n",
    "    try:\n",
    "        download_image(input['url'])\n",
    "        msg = 'downloaded image'\n",
    "    except Exception as e:\n",
    "        msg = 'failed to get image:' + str(e)\n",
    "    try: label = classify_image()\n",
    "    except Exception as e:\n",
    "        label = str(e)\n",
    "    output = {\n",
    "        'label': label,\n",
    "        'msg': msg\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fieldcady/Desktop/kube/foo\n",
      "[master e45bdd3] Code for the algorithm, from Jupyter\n",
      " Committer: field-cady <fieldcady@Fields-MacBook-Pro.local>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly:\n",
      "\n",
      "    git config --global user.name \"Your Name\"\n",
      "    git config --global user.email you@example.com\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 1 file changed, 4 insertions(+), 1 deletion(-)\n",
      "/Users/fieldcady/Desktop/kube\n",
      "/Users/fieldcady/Desktop/kube/foo\n",
      "On branch master\n",
      "Your branch is ahead of 'origin/master' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "/Users/fieldcady/Desktop/kube\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%cd $ALGORITHM_NAME\n",
    "!git commit -a -m \"Code for the algorithm, from Jupyter\"\n",
    "%cd ..\n",
    "%cd $ALGORITHM_NAME\n",
    "!git commit -a -m \"Code for the algorithm, from Jupyter\"\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fieldcady/Desktop/kube/foo\n",
      "Everything up-to-date\n",
      "/Users/fieldcady/Desktop/kube\n"
     ]
    }
   ],
   "source": [
    "# Occassionally you have to run this twice...\n",
    "%cd $ALGORITHM_NAME\n",
    "!git push\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compilation': {'output': None, 'successful': True},\n",
       " 'details': {'label': 'foo', 'summary': None, 'tagline': 'bar'},\n",
       " 'name': 'foo',\n",
       " 'resource_type': 'algorithm',\n",
       " 'self_link': None,\n",
       " 'settings': {'algorithm_callability': 'private',\n",
       "              'environment': None,\n",
       "              'language': None,\n",
       "              'license': 'apl',\n",
       "              'network_access': 'full',\n",
       "              'package_set': 'tensorflow-gpu-1.12',\n",
       "              'pipeline_enabled': False,\n",
       "              'royalty_microcredits': 0,\n",
       "              'source_visibility': 'open'},\n",
       " 'version_info': {'git_hash': 'dcf13b8bfe2b363f1ab9281d76ce81ad0bd7f2a4',\n",
       "                  'release_notes': None,\n",
       "                  'sample_input': None,\n",
       "                  'sample_output': None,\n",
       "                  'semantic_version': '1.0.6'}}"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Algorithmia, os\n",
    "api_key = os.environ['ALGORITHMIA_API_KEY']\n",
    "client = Algorithmia.client(api_key)\n",
    "client.algo(algo_namespace).compile()\n",
    "client.algo(algo_namespace).publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/tmp/': ['tmp5b0b4olo', 'algoout', 'unzipped_files', '.X11-unix', 'foo.png'], '/tmp/unzipped_files/model/': ['variables', 'saved_model.pb'], 'label': 0, 'msg': 'downloaded_fine', 'status': 'extracted,created,downloaded,labelled'}\n"
     ]
    }
   ],
   "source": [
    "algo_namespace = \"{}/{}\".format(ALGORITHMIA_USERNAME, ALGORITHM_NAME)\n",
    "latest_hash = client.algo(algo_namespace).info().version_info.git_hash\n",
    "algo_input = {\n",
    "    #\"url\": \"https://edwin-de-jong.github.io/blog/mnist-sequence-data/fig/5.png\"\n",
    "    \"url\": \"https://miro.medium.com/max/490/1*nlfLUgHUEj5vW7WVJpxY-g.png\"\n",
    "}\n",
    "res = client.algo(algo_namespace+'/'+latest_hash).pipe(algo_input).result\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
