{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Kubeflow with Algorithmia\n",
    "\n",
    "This notebook walks you through how to traing a machine learning model using Google's Kubeflow, and then import the model into Algorithmia and write an algorithm that incorporates it.  There are 5 steps in this workflow:\n",
    "\n",
    "* Installing the software you need to access google's cloud services, control kubeflow, etc\n",
    "* Setting up a project in Google's cloud, under which kubeflow will run\n",
    "* Create a kubernetes cluster that is associated with ths project and running kubeflow\n",
    "* Set up a storage bucket in google's cloud that is associated with the project\n",
    "* Create a docker image that can be used for training, by downloading the example code\n",
    "* Configure the training job and kick it off.  This will store a trained model in the bucket you created\n",
    "* Download the trained model, load it into Algorithmia's data API, and create an algorithm that uses it\n",
    "\n",
    "This tutorial is largely based on [Google's end-to-end Kubeflow tutorial](https://www.kubeflow.org/docs/gke/gcp-e2e/)  (which is archived [here](https://web.archive.org/save/https://www.kubeflow.org/docs/gke/gcp-e2e/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Installing Required Software\n",
    "\n",
    "We need to make the following software available on your computer:\n",
    "* gcloud: access to Google’s cloud services\n",
    "* kubectl: controlling a Kubernetes cluster\n",
    "* kustomize: a helper tool that makes it easier to modify kubernetes jobs\n",
    "* kfctl: controlling Kubeflow specifically\n",
    "\n",
    "First off though, we will be CDing in and out of some directories, so let's make life easier by creating a python variable that keeps track of out root directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The working directory is:/Users/fieldcady/Desktop/model-deployment/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#ROOTDIR = os.getcwd()\n",
    "ROOTDIR='/Users/fieldcady/Desktop/model-deployment/'\n",
    "print('The working directory is:' + ROOTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gcloud, Kubectl and kusomize\n",
    "In this tutorial there will be a lot of shell commands executed from within Jupyter.  Unfortunately, installing the first round of software requires to shell commands that we can **NOT** put in a cell to execute (they require some interactivity, restarting a shell, etc).  You will have to open a terminal program and run them from there.\n",
    "\n",
    "\n",
    "If you are on a Mac you will have to run the following from the command shell to instalol gcloud:\n",
    "```\n",
    "curl https://sdk.cloud.google.com | bash  # install gcloud\n",
    "exec -l $SHELL  # restart the shell\n",
    "gcloud init\n",
    "```\n",
    "If you are not on a Mac check out instructions for other systems [here](https://cloud.google.com/sdk/docs/downloads-interactive).\n",
    "\n",
    "Once you have  gcloud installed you can install kubectl and kustomize like this (again, on a Mac):\n",
    "```\n",
    "gcloud components install kubectl\n",
    "brew install kustomize\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kfctl\n",
    "We don’t “install” kfctl exactly - we just download the executable and put it in a place we can reference.  Download the appropriate version from the [Kubeflow releases page](https://github.com/kubeflow/kubeflow/releases/) into the directory of your choice, unzip it, and then put it on your path.  Example commands (in this case just putting it in the working directory) are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd $ROOTDIR\n",
    "!wget https://github.com/kubeflow/kubeflow/releases/download/v0.5.1/kfctl_v0.5.1_darwin.tar.gz\n",
    "!tar -xvf kfctl_v0.5.1_darwin.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted kfctl in the root dir\n"
     ]
    }
   ],
   "source": [
    "if 'kfctl' in os.listdir(ROOTDIR):\n",
    "    print('Extracted kfctl in the root dir')\n",
    "else: print('Error!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Creating the Project\n",
    "Google Cloud divides things into \"projects\" that can have multiple resources assocaited with them.  If you already have a project you can just use it.  In this section we will create a new project and configure gcloud to point to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use gcloud to tell Google who you are and associate all of this with your google email address.  This command will open up a browser window where you can log in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then go to the [Google Cloud console page](https://console.cloud.google.com), create a project, and get its ID.  Also **make sure billing is enabled for it**, and choose a geographical region and zone for the project.  Set those decisions as python variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='kubeflow-245520'\n",
    "REGION='us-west2'\n",
    "ZONE='us-west2-c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then configure gcloud to point to that project and zone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/zone].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT\n",
    "!gcloud config set compute/zone $ZONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Create GKE Cluster\n",
    "Now we will create a Google Kubernetes Engine (GKE) cluster, under the umbrella of the current project, that has kubeflow running on it.\n",
    "\n",
    "Choose a name for the Kubernetes cluster to use, a name for the GCloud deployment, and login credentials for the web UI of the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFAPP='kfapp7'\n",
    "KUBEFLOW_USERNAME='fcady'\n",
    "KUBEFLOW_PASSWORD='mypass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use kfctl to create this Kubeflow project and a directory dedicated to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KUBEFLOW_USERNAME=fcady\n",
      "env: KUBEFLOW_PASSWORD=mypass\n",
      "/Users/fieldcady/Desktop/model-deployment\n",
      "\u001b[36mINFO\u001b[0m[0005] Not skipping GCP project init, running gcpInitProject.  \u001b[36mfilename\u001b[0m=\"gcp/gcp.go:1619\"\n",
      "\u001b[33mWARN\u001b[0m[0008] batch API enabling is running: [deploymentmanager.googleapis.com servicemanagement.googleapis.com container.googleapis.com cloudresourcemanager.googleapis.com endpoints.googleapis.com file.googleapis.com ml.googleapis.com iam.googleapis.com sqladmin.googleapis.com] (op = operations/acf.d5cbac48-65c2-4828-b118-05fc6ab87eed)  \u001b[33mfilename\u001b[0m=\"gcp/gcp.go:1594\"\n",
      "\u001b[33mWARN\u001b[0m[0009] batch API enabling is running: [deploymentmanager.googleapis.com servicemanagement.googleapis.com container.googleapis.com cloudresourcemanager.googleapis.com endpoints.googleapis.com file.googleapis.com ml.googleapis.com iam.googleapis.com sqladmin.googleapis.com] (op = operations/acf.d5cbac48-65c2-4828-b118-05fc6ab87eed)  \u001b[33mfilename\u001b[0m=\"gcp/gcp.go:1594\"\n",
      "\u001b[33mWARN\u001b[0m[0010] batch API enabling is running: [deploymentmanager.googleapis.com servicemanagement.googleapis.com container.googleapis.com cloudresourcemanager.googleapis.com endpoints.googleapis.com file.googleapis.com ml.googleapis.com iam.googleapis.com sqladmin.googleapis.com] (op = operations/acf.d5cbac48-65c2-4828-b118-05fc6ab87eed)  \u001b[33mfilename\u001b[0m=\"gcp/gcp.go:1594\"\n",
      "\u001b[33mWARN\u001b[0m[0012] batch API enabling is running: [deploymentmanager.googleapis.com servicemanagement.googleapis.com container.googleapis.com cloudresourcemanager.googleapis.com endpoints.googleapis.com file.googleapis.com ml.googleapis.com iam.googleapis.com sqladmin.googleapis.com] (op = operations/acf.d5cbac48-65c2-4828-b118-05fc6ab87eed)  \u001b[33mfilename\u001b[0m=\"gcp/gcp.go:1594\"\n",
      "\u001b[33mWARN\u001b[0m[0014] batch API enabling is running: [deploymentmanager.googleapis.com servicemanagement.googleapis.com container.googleapis.com cloudresourcemanager.googleapis.com endpoints.googleapis.com file.googleapis.com ml.googleapis.com iam.googleapis.com sqladmin.googleapis.com] (op = operations/acf.d5cbac48-65c2-4828-b118-05fc6ab87eed)  \u001b[33mfilename\u001b[0m=\"gcp/gcp.go:1594\"\n",
      "\u001b[33mWARN\u001b[0m[0018] batch API enabling is running: [deploymentmanager.googleapis.com servicemanagement.googleapis.com container.googleapis.com cloudresourcemanager.googleapis.com endpoints.googleapis.com file.googleapis.com ml.googleapis.com iam.googleapis.com sqladmin.googleapis.com] (op = operations/acf.d5cbac48-65c2-4828-b118-05fc6ab87eed)  \u001b[33mfilename\u001b[0m=\"gcp/gcp.go:1594\"\n",
      "\u001b[36mINFO\u001b[0m[0024] batch API enabling is completed: [deploymentmanager.googleapis.com servicemanagement.googleapis.com container.googleapis.com cloudresourcemanager.googleapis.com endpoints.googleapis.com file.googleapis.com ml.googleapis.com iam.googleapis.com sqladmin.googleapis.com]  \u001b[36mfilename\u001b[0m=\"gcp/gcp.go:1590\"\n",
      "\u001b[36mINFO\u001b[0m[0024] reading from /Users/fieldcady/Desktop/model-deployment/kfapp7/app.yaml  \u001b[36mfilename\u001b[0m=\"coordinator/coordinator.go:341\"\n"
     ]
    }
   ],
   "source": [
    "%env KUBEFLOW_USERNAME=$KUBEFLOW_USERNAME\n",
    "%env KUBEFLOW_PASSWORD=$KUBEFLOW_PASSWORD\n",
    "%cd $ROOTDIR\n",
    "!$ROOTDIR/kfctl init $KFAPP --platform gcp --project $PROJECT --use_basic_auth -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that it all went well, check that there is a directory into the ROOTDIR that has the name of your Kubeflow project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!  You made the directory\n"
     ]
    }
   ],
   "source": [
    "contents = os.listdir(ROOTDIR)\n",
    "if KFAPP in contents: print('Success!  You made the directory')\n",
    "else: print('Darn, something screwed up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now CD into the directory and let's set up the cluster itself, allocating resources and all that.   This will fail if you don’t have billing enabled!  This process will take a while (maybe 20 minutes?), so now would be a good time to grab some coffee :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=\"/Users/fieldcady/Downloads/kubeflow-d7acf7627f84.json\"\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=\"/Users/fieldcady/Downloads/kubeflow-d7acf7627f84.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fieldcady/Desktop/model-deployment/kfapp7\n",
      "\u001b[36mINFO\u001b[0m[0000] deploying kubeflow application                \u001b[36mfilename\u001b[0m=\"cmd/apply.go:35\"\n",
      "\u001b[36mINFO\u001b[0m[0000] reading from /Users/fieldcady/Desktop/model-deployment/kfapp7/app.yaml  \u001b[36mfilename\u001b[0m=\"coordinator/coordinator.go:341\"\n",
      "\u001b[31mFATA\u001b[0m[0000] Could not authenticate Client: google: error getting credentials using GOOGLE_APPLICATION_CREDENTIALS environment variable: open \"/Users/fieldcady/Downloads/kubeflow-d7acf7627f84.json\": no such file or directory  \u001b[31mfilename\u001b[0m=\"gcp/gcp.go:101\"\n"
     ]
    }
   ],
   "source": [
    "%cd $ROOTDIR/$KFAPP\n",
    "#!$ROOTDIR/kfctl generate all -V --zone $ZONE\n",
    "!$ROOTDIR/kfctl apply all -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cluster starts it will have a URL where you can log in with the previously set username and password to check on everything.  This code snippet will print the url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://kfapp7.endpoints.kubeflow-245520.cloud.goog/\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://{KFAPP}.endpoints.{PROJECT}.cloud.goog/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next connect to Kubeflow cluster+deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for kfapp3.\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!gcloud container clusters get-credentials $KFAPP --zone $ZONE --project $PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Make Storage Bucket\n",
    "Now we will set up a cloud storage location where our trained model will be stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME=PROJECT + '-' + KFAPP + '-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west2-c gs://kubeflow-245520-kfapp6-bucket\n",
      "Creating gs://kubeflow-245520-kfapp6-bucket/...\n"
     ]
    }
   ],
   "source": [
    "!echo $ZONE gs://$BUCKET_NAME\n",
    "!gsutil mb -c regional -l $REGION gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the bucket listed at ```https://console.cloud.google.com/storage/browser?project=<PROJECT>```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Get code, build training Docker image, and Push it to the Registry\n",
    "The next step for getting Kubeflow running is to get the Docker training image built and stored in GCE where Kubeflow can find it.\n",
    "\n",
    "This is where you will start changing things for your own projects, but for this walk-through we will use the MNIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/kubeflow-245520/kfapp6-train:1567792362\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "VERSION_TAG=str(round(time.time()))\n",
    "TRAIN_IMG_PATH=f\"gcr.io/{PROJECT}/{KFAPP}-train:{VERSION_TAG}\"\n",
    "WORKING_DIR=ROOTDIR + \"examples/mnist\"\n",
    "print(TRAIN_IMG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd $ROOTDIR\n",
    "!git clone https://github.com/kubeflow/examples.git\n",
    "!docker build -f examples/mnist/Dockerfile.model -t $TRAIN_IMG_PATH examples/mnist\n",
    "!gcloud auth configure-docker --quiet\n",
    "!docker push $TRAIN_IMG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should then be able to see the image in the Image Registry in gcloud at the URL created by this code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.cloud.google.com/gcr/images/kubeflow-245520\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://console.cloud.google.com/gcr/images/{PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Create the training job and run it on the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we cd into the directory with the configuration files for training and use kustomize to make some edits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd $ROOTDIR/examples/mnist/training/GCS\n",
    "!kustomize edit add configmap mnist-map-training --from-literal=secretName=user-gcp-sa\n",
    "!kustomize edit add configmap mnist-map-training --from-literal=secretMountPath=/var/secrets\n",
    "!kustomize edit add configmap mnist-map-training --from-literal=GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/user-gcp-sa.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we **should** be one command away from launching the training job on our Kubeflow cluster.\n",
    "\n",
    "Alas, Kubeflow is an early-stage open-source project, and that means some rough edges.  In this case there are bugs that improperly handle the example configuration files, and we will need to edit them ourselves.  The two files that need editing are kustomization.yaml in the current directory (\"main file\") and kustomization.yaml in the base directory (\"base file\").  The changes that need to be made are:\n",
    "* In the main file the file Chief_patch.yaml gets added as a path.  But it needs a namespace associated with it that matches the one in the base file.  Do that by adding “namespace: kubeflow” to the properties underneath it.\n",
    "* The main and base files both have a \"vars:\" section that defines some properties.  Move all of the properties in the base file to the main file, deleting that section in the base file.\n",
    "\n",
    "For simplicity, the next cell will write versions of these files with those changes already made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fieldcady/Desktop/model-deployment/kubeflow_integration/examples/mnist/training/GCS\n"
     ]
    }
   ],
   "source": [
    "%cd $ROOTDIR/examples/mnist/training/GCS\n",
    "!git checkout ../base/kustomization.yaml\n",
    "!git checkout kustomization.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file = \"\"\"apiVersion: kustomize.config.k8s.io/v1beta1\n",
    "kind: Kustomization\n",
    "\n",
    "resources:\n",
    "- Chief.yaml\n",
    "\n",
    "namespace: kubeflow\n",
    "\n",
    "generatorOptions:\n",
    "  disableNameSuffixHash: true\n",
    "\n",
    "configurations:\n",
    "- params.yaml\n",
    "\"\"\"\n",
    "\n",
    "main_file = f\"\"\"apiVersion: kustomize.config.k8s.io/v1beta1\n",
    "kind: Kustomization\n",
    "\n",
    "\n",
    "configurations:\n",
    "- params.yaml\n",
    "\n",
    "# TBD (jinchihe) Need move the image to base file once.\n",
    "# the issue addressed: kubernetes-sigs/kustomize/issues/1040\n",
    "# TBD (jinchihe) Need to update the image once\n",
    "# the issue addressed: kubeflow/testing/issues/373\n",
    "images:\n",
    "- name: training-image\n",
    "  newName: {TRAIN_IMG_PATH}\n",
    "\n",
    "vars:\n",
    "- fieldref:\n",
    "    fieldPath: data.name\n",
    "  name: trainingName\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.modelDir\n",
    "  name: modelDir\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.exportDir\n",
    "  name: exportDir\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.trainSteps\n",
    "  name: trainSteps\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.batchSize\n",
    "  name: batchSize\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.learningRate\n",
    "  name: learningRate\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.GOOGLE_APPLICATION_CREDENTIALS\n",
    "  name: GOOGLE_APPLICATION_CREDENTIALS\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.secretName\n",
    "  name: secretName\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "- fieldref:\n",
    "    fieldPath: data.secretMountPath\n",
    "  name: secretMountPath\n",
    "  objref:\n",
    "    apiVersion: v1\n",
    "    kind: ConfigMap\n",
    "    name: mnist-map-training\n",
    "\n",
    "patchesJson6902:\n",
    "- path: Chief_patch.yaml\n",
    "  target:\n",
    "    group: kubeflow.org\n",
    "    kind: TFJob\n",
    "    name: $(trainingName)\n",
    "    namespace: kubeflow\n",
    "    version: v1beta2\n",
    "resources:\n",
    "- ../base\n",
    "configMapGenerator:\n",
    "- literals:\n",
    "  - name=mnist-train-dist5\n",
    "  - trainSteps=2000\n",
    "  - batchSize=1000\n",
    "  - learningRate=0.01\n",
    "  - secretName=user-gcp-sa\n",
    "  - secretMountPath=/var/secrets\n",
    "  - GOOGLE_APPLICATION_CREDENTIALS=/var/secrets/user-gcp-sa.json\n",
    "  - modelDir=gs://{BUCKET_NAME}/\n",
    "  - exportDir=gs://{BUCKET_NAME}/export\n",
    "  name: mnist-map-training\n",
    "\"\"\"\n",
    "\n",
    "_=open('../base/kustomization.yaml', 'w').write(base_file)\n",
    "_=open('kustomization.yaml', 'w').write(main_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the name=mnist-train-dist line in the YAML file.  **You will have to choose a new name every\n",
    "time you re-kick-off the training job** - otherwise the workflow will not get created.  Now we can kick off the job with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap \"mnist-map-training-bdgcg76k57\" created\n",
      "tfjob.kubeflow.org \"mnist-train-dist5\" configured\n"
     ]
    }
   ],
   "source": [
    "!kustomize build . | kubectl apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can monitor the workflow on the [Google Worklow Page](https://console.cloud.google.com/kubernetes/workload), where you should see a workflow like \"mnist-train-dist-chief-0\".  **Wait for it to finish.**  After it finishes running then you can copy the exported model file (which is a SavedModel in Tensorflow) to your local machine and zip it up for transfer to Algorithmia.\n",
    "\n",
    "Now is a good time to grab some coffe - this might take a while. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Copy trained model to local space\n",
    "!gsutil cp -r gs://$BUCKET_NAME/export .\n",
    "# Make model/ directory, deleting if it already exists\n",
    "!rm -rf model*\n",
    "!mkdir model\n",
    "# Compress the trained model into a ZIP file\n",
    "# NOTE: this expects there to be only one model in yoru export/ folder\n",
    "!cp -r export/$(ls export)/* model/\n",
    "!zip model.zip -r model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: Loading Model to Algorithmia and Creating an Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Put zipped model into Algorithmia\n",
    "!algo rm .my/kubeflow_example force=true\n",
    "!algo mkdir .my/kubeflow_example\n",
    "!algo cp model.zip data://.my/kubeflow_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then go to [Algorithmia](http://www.algorithmia.com), create a new algorithm (**making sure to give it internet access** for this tutorial), and set its name as a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMIA_USERNAME='fcady'\n",
    "ALGORITHM_NAME=\"foo\"\n",
    "\n",
    "FILE_TO_WRITE=ALGORITHM_NAME+'/src/'+ALGORITHM_NAME+'.py'\n",
    "FILE_TO_COMMIT='src/'+ALGORITHM_NAME+'.py'\n",
    "REPO_TO_CLONE=ALGORITHMIA_USERNAME+'/'+ALGORITHM_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use git to checkout this algorithm, write some example code that uses out model, and push it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd $ROOTDIR\n",
    "!rm -rf $ALGORITHM_NAME  # Delete previously downloaded clone, if it exists\n",
    "!algo clone $REPO_TO_CLONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting foo/src/foo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $FILE_TO_WRITE\n",
    "import Algorithmia\n",
    "import tensorflow as tf\n",
    "import requests, zipfile\n",
    "from os import mkdir, listdir\n",
    "\n",
    "IMAGE_FNAME  = '/tmp/foo.png'\n",
    "\n",
    "client = Algorithmia.client()\n",
    "\n",
    "def extract_model():\n",
    "    filename = \"data://.my/kubeflow_example/model.zip\"\n",
    "    input_zip = client.file(filename).getFile().name\n",
    "    mkdir(\"/tmp/unzipped_files\")\n",
    "    zipped_file = zipfile.ZipFile(input_zip)\n",
    "    return zipped_file.extractall(\"/tmp/unzipped_files\")\n",
    "\n",
    "def download_image(url):\n",
    "    with open(IMAGE_FNAME, 'wb') as f:\n",
    "        f.write(requests.get(url).content)\n",
    "\n",
    "def create_session(path_to_graph = \"/tmp/unzipped_files/model\"):\n",
    "    session = tf.Session()\n",
    "    tf.saved_model.loader.load(session, ['serve'], path_to_graph)\n",
    "    y = session.graph.get_tensor_by_name('Softmax:0')\n",
    "    x = session.graph.get_tensor_by_name('Placeholder:0')\n",
    "    return (y, x, session)\n",
    "\n",
    "extract_model()\n",
    "Y, X, SESSION = create_session()\n",
    "\n",
    "def classify_image():\n",
    "    img = tf.keras.preprocessing.image.load_img(IMAGE_FNAME).resize((28,28))\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    xx = x.mean(axis=2).reshape((1,28,28)) / 255\n",
    "    predict_values = tf.argmax(Y, 1)\n",
    "    ret = predict_values.eval(session=SESSION,feed_dict={X: xx})\n",
    "    return int(ret)\n",
    "\n",
    "def apply(input):\n",
    "    try:\n",
    "        download_image(input['url'])\n",
    "        msg = 'downloaded image'\n",
    "    except Exception as e:\n",
    "        msg = 'failed to get image:' + str(e)\n",
    "    try: label = classify_image()\n",
    "    except Exception as e:\n",
    "        label = str(e)\n",
    "    output = {\n",
    "        'label': label,\n",
    "        'msg': msg\n",
    "    }\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Committing and pushing our changes can be a bit finicky when done through Jupyter.  Specifically:\n",
    "* We commit the changes twice, because the first commit doesn't always take\n",
    "* Sometimes you may have to kill the push process and restart it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd $ROOTDIR/$ALGORITHM_NAME\n",
    "!git commit -a -m \"Code for the algorithm, from Jupyter\"\n",
    "%cd $ROOTDIR/$ALGORITHM_NAME\n",
    "!git commit -a -m \"Code for the algorithm, from Jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fieldcady/Desktop/model-deployment/kubeflow_integration/kfapp3/examples/mnist/training/GCS/foo\n",
      "Everything up-to-date\n",
      "/Users/fieldcady/Desktop/model-deployment/kubeflow_integration/kfapp3/examples/mnist/training/GCS\n"
     ]
    }
   ],
   "source": [
    "# Occassionally you have to run this twice...\n",
    "%cd $ROOTDIR/$ALGORITHM_NAME\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Algorithmia, os\n",
    "api_key = os.environ['ALGORITHMIA_API_KEY']  # Or enter the key directly\n",
    "client = Algorithmia.client(api_key)\n",
    "\n",
    "algo_namespace = \"{}/{}\".format(ALGORITHMIA_USERNAME, ALGORITHM_NAME)\n",
    "client.algo(algo_namespace).compile()\n",
    "client.algo(algo_namespace).publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 8, 'msg': 'downloaded image'}\n"
     ]
    }
   ],
   "source": [
    "latest_hash = client.algo(algo_namespace).info().version_info.git_hash\n",
    "algo_input = {\n",
    "    \"url\": \"https://edwin-de-jong.github.io/blog/mnist-sequence-data/fig/5.png\"\n",
    "    #\"url\": \"https://miro.medium.com/max/490/1*nlfLUgHUEj5vW7WVJpxY-g.png\"\n",
    "}\n",
    "res = client.algo(algo_namespace+'/'+latest_hash).pipe(algo_input).result\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
